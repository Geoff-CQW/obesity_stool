{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('metagenomics/abundance_stoolsubset.csv', dtype='str')\n",
    "cols = data.columns\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### preprocess data ###\n",
    "\n",
    "# filter for categories of interest\n",
    "\n",
    "processed = data[data['disease'].isin(['obesity', 'obese', 'overweight','leaness', 'n'])].copy()\n",
    "\n",
    "# drop columns that are not needed\n",
    "to_drop = list(cols[2:4]) + list(cols[8:20]) + list(cols[21:211])\n",
    "processed.drop(columns = to_drop, inplace = True)\n",
    "\n",
    "# remove samples without bmi, convert bmi to float and filter for regular weight and obese\n",
    "processed = processed[~processed['bmi'].isin(['na', 'nd'])]\n",
    "\n",
    "processed['bmi'] = pd.to_numeric(processed['bmi'], downcast ='float')\n",
    "\n",
    "processed = processed[(processed['bmi']  <= 25) | (processed['bmi'] >= 30)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second filtering round, see description below for why\n",
    "bacteria = list(processed.columns)[7:]\n",
    "\n",
    "s = re.compile(r's__\\w+$')\n",
    "\n",
    "not_species = [i for i in bacteria if not s.search(i)]\n",
    "\n",
    "processed.drop(columns = not_species, inplace=True)\n",
    "\n",
    "\n",
    "# create a new column as labels\n",
    "processed['label'] = processed['bmi'].apply(lambda x:  0 if (x < 25) else 1)\n",
    "\n",
    "processed.reset_index(inplace=True, drop=True)\n",
    "\n",
    "species = processed.columns[7:833]\n",
    "\n",
    "processed[species] = processed[species].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename feature names to shorter one\n",
    "\n",
    "s = re.compile(r's__(\\w+)')\n",
    "\n",
    "short = []\n",
    "\n",
    "for i in list(species):\n",
    "    short.append( s.search(i).group(1).replace(\"_\", \" \"))\n",
    "    \n",
    "new = dict(zip(species, short))\n",
    "\n",
    "processed.rename(columns = new, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using BMI to separate into obese and normal. Also should note that the bacteria \n",
    "\n",
    "The k/p/c is actually the classification category for the bacteria, c = class, g = genus, o = order, f = family, s = species.\n",
    "\n",
    "So the bacteria species columns contains a lot of redundant data that are correlated, so have to remove them.\n",
    "\n",
    "e.g. for Archaea, there are multiple columns, k__archaea is actually the total count of all the archaea detected and then p__Euryarchaeota is the sum of all the classes/genus belonging to these classes. \n",
    "\n",
    "use RE to filter for the final level, which is species then type, then filter the columns again to cut down on the features.\n",
    "\n",
    "t__GCT.. is actually the refseq assembly accession number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "\n",
    "# shuffle data\n",
    "#processed = processed.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "x = processed.iloc[:, 7:833]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "y = processed['label']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 63)\n",
    "                                                   \n",
    "\n",
    "#y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(pop)\n",
    "\n",
    "test = dict(c)\n",
    "\n",
    "dict(sorted(test.items(), key=lambda item: -item[1]))\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('dict.csv', 'w') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in test.items():\n",
    "       writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(model, x, y, n=50):\n",
    "    ''' does 50 train test splits and calculates model metrics'''\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    report = metrics.classification_report(y_test, model.predict(x_test), output_dict=True)\n",
    "    \n",
    "    report_std = {}\n",
    "    \n",
    "    for i in report.keys():\n",
    "        if i == 'accuracy':\n",
    "            report_std[i] = []\n",
    "        else:\n",
    "            report_std[i] = {}\n",
    "            for j in report[i].keys():\n",
    "                report_std[i][j] = []\n",
    "\n",
    "    \n",
    "    scoring = ['balanced_accuracy', 'accuracy', 'f1','precision','recall','roc_auc']\n",
    "\n",
    "    cross_val ={'balanced_accuracy':0, 'accuracy':0, 'f1':0,'precision':0,'recall':0,'roc_auc':0}\n",
    "        \n",
    "    cross_val_std ={'balanced_accuracy':[], 'accuracy':[], 'f1':[],'precision':[],'recall':[],'roc_auc':[]}\n",
    "    \n",
    "    for count in range(n):\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        test = metrics.classification_report(y_test, model.predict(x_test), output_dict=True)\n",
    "        \n",
    "        for i in report.keys():\n",
    "            if i == 'accuracy':\n",
    "                report[i] += test[i]\n",
    "                report_std[i].append(test[i])\n",
    "            else:\n",
    "                for j in report[i].keys():\n",
    "                    report[i][j] += test[i][j]\n",
    "                    report_std[i][j].append(test[i][j])\n",
    "       \n",
    "        for p in scoring:\n",
    "            scores = cross_val_score(model, x_train, y_train, scoring = p)\n",
    "\n",
    "            cross_val[p] += np.mean(scores)\n",
    "            \n",
    "            cross_val_std[p].append((np.mean(scores)))\n",
    "            \n",
    "    \n",
    "    for i in scoring:\n",
    "        print(i)\n",
    "        print('mean: %0.3f' % (cross_val[i]/n))\n",
    "        print('std dev: %0.3f' % (np.std(cross_val_std[i])))\n",
    "        print()\n",
    "        \n",
    "    for i in report.keys():\n",
    "        if i == 'accuracy':\n",
    "            report[i] = report[i]/(n+1)\n",
    "            report_std[i] = np.std(report_std[i])\n",
    "        else:\n",
    "            for j in report[i].keys():\n",
    "                report[i][j] = report[i][j]/(n+1)\n",
    "                report_std[i][j] = np.std(report_std[i][j])\n",
    "        \n",
    "    return report, report_std\n",
    "\n",
    "\n",
    "# def model_performance(model, x, y):\n",
    "#     ''' 20 fold cross val score'''\n",
    "#     scoring = ['balanced_accuracy', 'accuracy', 'f1','precision','recall','roc_auc']\n",
    "    \n",
    "#     for p in scoring:\n",
    "#         scores = cross_val_score(model, x, y, cv=20, scoring = p)\n",
    "#         print(p)\n",
    "#         print('mean: %0.3f' % np.mean(scores))\n",
    "#         print('std dev: %0.2f' % np.std(scores))\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation plot of features\n",
    "\n",
    "corr = processed.iloc[:, 7:833].corr()\n",
    "corr.fillna(0, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,8))\n",
    "\n",
    "sns.heatmap(corr, xticklabels = False, yticklabels = False, ax=ax, cmap='viridis')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('correlation_heatmap.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log regression ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "log_params = {'penalty':['l1', 'l2']}\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000, solver = 'liblinear')\n",
    "\n",
    "log_reg_CV = GridSearchCV(log_reg, log_params, scoring = 'balanced_accuracy').fit(x_train, y_train)\n",
    "\n",
    "log_reg_CV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log_params = {'C':list(np.linspace(0.001, 5, num=20))}\n",
    "\n",
    "log_reg = LogisticRegression(penalty = 'l1', solver = 'liblinear', max_iter=1000)\n",
    "\n",
    "log_reg_CV = GridSearchCV(log_reg, log_params, scoring = 'balanced_accuracy').fit(x_train, y_train)\n",
    "\n",
    "log_reg_CV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(penalty = 'l1', solver = 'liblinear', C = 0.264)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report, report_std = model_performance(log_reg, x, y, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.264, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "# print(metrics.classification_report(y_test, log_reg.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#linear SVM\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linsvm_params = {'penalty':['l1','l2'], 'C':list(np.linspace(0.01, 10, num=10))}\n",
    "\n",
    "lin_svm = LinearSVC(max_iter = 2000)\n",
    "\n",
    "linsvm_cv = GridSearchCV(lin_svm, linsvm_params, scoring = 'balanced_accuracy').fit(x_train, y_train)\n",
    "\n",
    "linsvm_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linsvm_params = {'C':list(np.linspace(0.001, 0.1, num=10))}\n",
    "\n",
    "lin_svm = LinearSVC(penalty = 'l2', max_iter = 2000)\n",
    "\n",
    "linsvm_cv = GridSearchCV(lin_svm, linsvm_params, scoring = 'balanced_accuracy').fit(x_train, y_train)\n",
    "\n",
    "linsvm_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lin_svm = LinearSVC(penalty = 'l2', C = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_performance(lin_svm, x_train, y_train)\n",
    "\n",
    "report, report_std = model_performance(lin_svm, x, y, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.001)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_svm.fit(x_train, y_train)\n",
    "\n",
    "# print(metrics.classification_report(y_test, lin_svm.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVM ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#step by step optimization\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_params = {'kernel':['rbf','sigmoid','poly'], 'gamma':list(np.logspace(-5,5, num=13)), 'C':list(np.logspace(-3,8, num=13))}\n",
    "\n",
    "svm = SVC(max_iter = 5000)\n",
    "\n",
    "svm_cv = GridSearchCV(svm, svm_params, scoring = 'balanced_accuracy', n_jobs = 2).fit(x_train, y_train)\n",
    "\n",
    "svm_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel = 'rbf', C = 38.3, gamma = 0.0032)\n",
    "\n",
    "# model_performance(svm, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report, report_std = model_performance(svm, x, y, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=38.3, gamma=0.0032)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(x_train, y_train)\n",
    "\n",
    "# print(metrics.classification_report(y_test, svm.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_params = {'criterion':['gini','entropy'], 'max_depth':[p for p in range(50)[5:]], 'min_samples_leaf':list(np.linspace(0.0001,0.25,num=50)), \n",
    "               'max_features':['sqrt','log2','None']}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tree_cv = GridSearchCV(tree, tree_params, scoring = 'balanced_accuracy').fit(x_train, y_train)\n",
    "\n",
    "tree_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy', max_depth = 39, max_features = 'sqrt', min_samples_leaf = 0.0001)\n",
    "\n",
    "# model_performance(tree, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report, report_std = model_performance(tree, x, y, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=39, max_features='sqrt',\n",
       "                       min_samples_leaf=0.0001)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.fit(x_train, y_train)\n",
    "\n",
    "# print(metrics.classification_report(y_test, tree.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "forest_params = {'n_estimators':[i for i in range(110)[10::20]], 'max_depth':[j for j in range(81)[5::15]], \n",
    "                 'min_samples_leaf':list(np.linspace(0.001,0.25,num=50)), 'max_features':['sqrt','log2']}\n",
    "\n",
    "forest_cv = GridSearchCV(forest, forest_params, scoring = 'balanced_accuracy', n_jobs=2).fit(x_train, y_train)\n",
    "\n",
    "forest_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forest = RandomForestClassifier(max_features = 'sqrt')\n",
    "\n",
    "forest_params = {'n_estimators':[i for i in range(80)[65:75]], 'max_depth':[j for j in range(80)[60:75:2]], \n",
    "                 'min_samples_leaf':list(np.linspace(0.0001,0.1,num=10))}\n",
    "\n",
    "forest_cv = GridSearchCV(forest, forest_params, scoring = 'balanced_accuracy').fit(x_train, y_train)\n",
    "\n",
    "forest_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 65, min_samples_leaf=0.0001, max_features='sqrt', max_depth = 62)\n",
    "\n",
    "# model_performance(forest, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report, report_std = model_performance(forest, x, y, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=62, max_features='sqrt',\n",
       "                       min_samples_leaf=0.0001, n_estimators=65)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(x_train, y_train)\n",
    "\n",
    "# print(metrics.classification_report(y_test, forest.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naive = GaussianNB()\n",
    "\n",
    "# model_performance(naive, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive.fit(x_train, y_train)\n",
    "\n",
    "print(metrics.classification_report(y_test, naive.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "ada_params = {'n_estimators':[i for i in range(150)[100:130]]}\n",
    "\n",
    "ada_cv = GridSearchCV(ada, ada_params, scoring = 'balanced_accuracy', n_jobs=2).fit(x_train, y_train)\n",
    "\n",
    "ada_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators = 117)\n",
    "\n",
    "# model_performance(ada, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report, report_std = model_performance(ada, x, y, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=117)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.fit(x_train, y_train)\n",
    "\n",
    "# print(metrics.classification_report(y_test, ada.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad = GradientBoostingClassifier()\n",
    "\n",
    "grad_params = {'n_estimators':[i for i in range(200)[100:300:20]]}\n",
    "\n",
    "grad_cv = GridSearchCV(grad, grad_params).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad = GradientBoostingClassifier()\n",
    "\n",
    "grad_params = {'n_estimators':[i for i in range(200)[125:150:5]], 'max_depth':[i for i in range(16)[3::2]]}\n",
    "\n",
    "grad_cv = GridSearchCV(grad, grad_params).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad = GradientBoostingClassifier(n_estimators = 145, max_depth = 5)\n",
    "\n",
    "grad_params = {'max_features':['sqrt','log2','auto','None']}\n",
    "\n",
    "grad_cv = GridSearchCV(grad, grad_params).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad = GradientBoostingClassifier(n_estimators = 145, max_depth = 5)\n",
    "\n",
    "# model_performance(grad, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = GradientBoostingClassifier()\n",
    "\n",
    "report, report_std = model_performance(grad, x, y, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=5, n_estimators=145)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.fit(x_train, y_train)\n",
    "\n",
    "# print(metrics.classification_report(y_test, grad.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (22,12))\n",
    "\n",
    "#plot ROC curve\n",
    "\n",
    "metrics.plot_roc_curve(log_reg, x_test, y_test, ax = ax[0], name = \"Log Reggression\")\n",
    "metrics.plot_roc_curve(lin_svm, x_test, y_test, ax = ax[0], name = \"Linear SVM\")\n",
    "metrics.plot_roc_curve(svm, x_test, y_test, ax = ax[0], name = \"kernel SVM\")\n",
    "# metrics.plot_roc_curve(naive, x_test, y_test, ax = ax[0], name = \"Naive Bayes\")\n",
    "metrics.plot_roc_curve(tree, x_test, y_test, ax = ax[0], name = \"Decision Tree\")\n",
    "metrics.plot_roc_curve(forest, x_test, y_test, ax = ax[0], name = \"Random Forest\", ls = \"-.\", color='r', linewidth = 2)\n",
    "metrics.plot_roc_curve(ada, x_test, y_test, ax = ax[0], name = \"Adaboost\")\n",
    "metrics.plot_roc_curve(grad, x_test, y_test, ax = ax[0], name = \"Gradient Boosting\", ls = '--', color='black', linewidth=2)\n",
    "ax[0].tick_params(axis='both', labelsize= 14)\n",
    "ax[0].set_xlabel('False Postive Rate', fontsize = 24)\n",
    "ax[0].set_ylabel('True Postive Rate', fontsize = 24)\n",
    "ax[0].text(-0.08, 1.065, \"A\", fontsize=24, fontweight='bold', va='top', ha='right')\n",
    "ax[0].legend(fontsize = 14)\n",
    "\n",
    "#plot precision-recall curve\n",
    "\n",
    "metrics.plot_precision_recall_curve(log_reg, x_test, y_test, ax = ax[1], name = \"Log Reggression\")\n",
    "metrics.plot_precision_recall_curve(lin_svm, x_test, y_test, ax = ax[1], name = \"Linear SVM\")\n",
    "metrics.plot_precision_recall_curve(svm, x_test, y_test, ax = ax[1], name = \"kernel SVM\")\n",
    "# metrics.plot_precision_recall_curve(naive, x_test, y_test, ax = ax[1], name = \"Naive Bayes\")\n",
    "metrics.plot_precision_recall_curve(tree, x_test, y_test, ax = ax[1], name = \"Decision Tree\")\n",
    "metrics.plot_precision_recall_curve(forest, x_test, y_test, ax = ax[1], name = \"Random Forest\", ls = \"-.\", color='r', linewidth = 2)\n",
    "metrics.plot_precision_recall_curve(ada, x_test, y_test, ax = ax[1], name = \"Adaboost\")\n",
    "metrics.plot_precision_recall_curve(grad, x_test, y_test, ax = ax[1], name = \"Gradient Boosting\", ls = '--', color='black', linewidth=2)\n",
    "ax[1].set_xlabel('Recall', fontsize = 24)\n",
    "ax[1].set_ylabel('Precision', fontsize = 24)\n",
    "ax[1].legend(loc = 'lower left',fontsize=14)\n",
    "ax[1].tick_params(axis='both', labelsize= 16)\n",
    "ax[1].text(-0.08, 1.035, \"B\", fontsize=24, fontweight='bold', va='top', ha='right')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance',dpi=300)\n",
    "\n",
    "\n",
    "#always put savefig before show(), if not will save empty image.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe of permutation importance of better performing models ###\n",
    "\n",
    "### Also the plots ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# forest_importance = permutation_importance(forest, x_test, y_test, scoring = 'balanced_accuracy', n_jobs = 2, random_state = 42)\n",
    "\n",
    "# ada_importance = permutation_importance(ada, x_test, y_test, scoring = 'balanced_accuracy', n_jobs = 2, random_state = 42)\n",
    "\n",
    "grad_importance = permutation_importance(grad, x_test, y_test, scoring = 'balanced_accuracy', n_jobs = 2, n_repeats = 100, random_state = 42)\n",
    "\n",
    "# svm_importance = permutation_importance(svm, x_test, y_test, scoring = 'balanced_accuracy', n_jobs = 2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = list(processed.columns[7:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perm_impt = pd.DataFrame(columns = ['Features', 'Forest','Grad', 'Ada', 'SVM'])\n",
    "\n",
    "# perm_impt['Features'] = features\n",
    "\n",
    "# perm_impt['Forest'] = forest_importance['importances_mean']\n",
    "\n",
    "# perm_impt['Ada'] = ada_importance['importances_mean']\n",
    "\n",
    "# perm_impt['Grad'] = grad_importance['importances_mean']\n",
    "\n",
    "# perm_impt['SVM'] = svm_importance['importances_mean']\n",
    "\n",
    "#perm_impt.to_csv('permutation_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perm_impt = pd.read_csv('feature_importance.csv')\n",
    "\n",
    "# forest_impt = list(perm_impt['Features'][perm_impt['Forest'] > 0])\n",
    "\n",
    "# ada_impt = list(perm_impt['Features'][perm_impt['Ada'] > 0])\n",
    "\n",
    "# grad_impt = list(perm_impt['Features'][perm_impt['Grad'] > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_count = []\n",
    "\n",
    "# for model in ['Forest','Grad', 'Ada']:\n",
    "#     feature_count.append((perm_impt[model] > 0).value_counts()[True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib_venn import venn2\n",
    "\n",
    "# forest_impt = list(perm_impt['Features'][perm_impt['Forest'] > 0])\n",
    "# grad_impt = list(perm_impt['Features'][perm_impt['Grad'] > 0])\n",
    "# ada_impt = list(perm_impt['Features'][perm_impt['Ada'] > 0])\n",
    "# svm_impt = list(perm_impt['Features'][perm_impt['SVM'] > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_labels = ['Random Forest','Gradient Boost', 'AdaBoost']\n",
    "\n",
    "# # Plot the figure.\n",
    "# fig, ax = plt.subplots(1,1, figsize = (14,10), facecolor = 'white')\n",
    "\n",
    "# ax = pd.Series(feature_count).plot(kind='bar')\n",
    "# ax.set_title('No. of Important Features', fontsize = 18, fontweight = 'bold', pad = 10)\n",
    "# ax.set_xlabel('Classifier', fontsize = 16, fontweight = 'bold', labelpad = 10)\n",
    "# ax.set_ylabel('Count',  fontsize = 16, fontweight = 'bold', labelpad = 10)\n",
    "# ax.set_xticklabels(x_labels, rotation = 0, fontsize = 14)\n",
    "\n",
    "# rects = ax.patches\n",
    "\n",
    "# # Make some labels.\n",
    "\n",
    "# for rect, label in zip(rects, feature_count):\n",
    "#     height = rect.get_height()\n",
    "#     ax.text(rect.get_x() + rect.get_width() / 2, height + 1, label,\n",
    "#             ha='center', va='bottom', fontsize = 14)\n",
    "\n",
    "# plt.savefig('impt_feature_count.png', dpi=150)\n",
    "    \n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_impt = pd.read_csv('dict_diabetes.csv', names=['features', 'count'])\n",
    "\n",
    "grad_impt = pd.read_csv('dict_obs.csv', names=['features', 'count'])\n",
    "\n",
    "cirr_impt = pd.read_csv('dict_cirrhosis.csv', names=['features', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from venn import venn\n",
    "\n",
    "test = {'Obesity':set(grad_impt['features']), 'Diabetes':set(diabetes_impt['features'])} #, 'Cirrhosis':set(cirr_impt['features'])}\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (14,10), facecolor='white')\n",
    "\n",
    "venn(test, fontsize = 18, legend_loc = 'lower left', ax=ax)\n",
    "\n",
    "#ax.set_title('Feature Overlap between Classifiers', fontsize = 18, fontweight ='bold', y=0.90)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('venn_diagram.png', dpi=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost analysis ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#grad_perm.to_csv('grad_perm_impt.csv', index=False)\n",
    "\n",
    "grad_perm = pd.read_csv('obs_avg_impt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "impt_score = np.zeros((826,))\n",
    "\n",
    "for i in range(50):\n",
    "    # split data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "\n",
    "    grad = GradientBoostingClassifier(n_estimators = 145, max_depth = 5).fit(x_train, y_train)\n",
    "    \n",
    "    perm_impt = permutation_importance(grad, x_test, y_test, scoring='balanced_accuracy', n_jobs = 2)\n",
    "    \n",
    "    # filter dataset\n",
    "    \n",
    "    impt_score += perm_impt['importances_mean']\n",
    "    \n",
    "temp1 = pd.DataFrame(columns = ['features','impt'])\n",
    "    \n",
    "temp1['features'] = short\n",
    "temp1['impt'] = impt_score/50\n",
    "    \n",
    "temp2 = temp1[temp1['impt'] > 0].copy()\n",
    "temp2.sort_values(by='impt', ascending=False, inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>impt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Klebsiella pneumoniae</td>\n",
       "      <td>0.022613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bacteroides intestinalis</td>\n",
       "      <td>0.010920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bacteroides stercoris</td>\n",
       "      <td>0.005639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bacteroides pectinophilus</td>\n",
       "      <td>0.003882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Streptococcus thermophilus</td>\n",
       "      <td>0.003866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ruminococcus obeum</td>\n",
       "      <td>0.003856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bacteroides massiliensis</td>\n",
       "      <td>0.003327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sutterella wadsworthensis</td>\n",
       "      <td>0.003230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Prevotella copri</td>\n",
       "      <td>0.002498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Barnesiella intestinihominis</td>\n",
       "      <td>0.002216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       features      impt\n",
       "0         Klebsiella pneumoniae  0.022613\n",
       "1      Bacteroides intestinalis  0.010920\n",
       "2         Bacteroides stercoris  0.005639\n",
       "3     Bacteroides pectinophilus  0.003882\n",
       "4    Streptococcus thermophilus  0.003866\n",
       "5            Ruminococcus obeum  0.003856\n",
       "6      Bacteroides massiliensis  0.003327\n",
       "7     Sutterella wadsworthensis  0.003230\n",
       "8              Prevotella copri  0.002498\n",
       "9  Barnesiella intestinihominis  0.002216"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2.to_csv('obs_avg_impt.csv', index_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of impt bacteria and permutation importance\n",
    "\n",
    "# grad_perm = pd.DataFrame(columns = ['Features','importances_mean', 'importances_std'])\n",
    "# grad_perm['Features'] = species\n",
    "# grad_perm['importances_mean'] = grad_importance['importances_mean']\n",
    "# grad_perm['importances_std'] = grad_importance['importances_std']\n",
    "# grad_perm = grad_perm[grad_perm['importances_mean'] > 0].copy()\n",
    "# grad_perm.sort_values(by='importances_mean', ascending=False, inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Streptococcus australis'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = list(temp2[:11]['features'])\n",
    "\n",
    "test.pop(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with all data including BMI, country etc\n",
    "\n",
    "stuff = list(processed.columns[:7]) + test + ['label']\n",
    "\n",
    "grad_stuff = processed[stuff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing labels for control and obese\n",
    "\n",
    "#grad_stuff = grad_means.copy()\n",
    "\n",
    "grad_stuff['label'][grad_stuff['label'] == 1] = 'Obese'\n",
    "\n",
    "grad_stuff['label'][grad_stuff['label'] == 0] = 'Ctrl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean species abundance for each species\n",
    "\n",
    "means = grad_stuff.groupby(by = 'label').mean().iloc[:,1:]\n",
    "\n",
    "# create a dataframe for mean abundance\n",
    "\n",
    "grad_means = pd.DataFrame(columns = ['Bacteria','Ctrl','Obese'])\n",
    "\n",
    "grad_means['Bacteria'] = test\n",
    "grad_means['Ctrl'] = list(means.iloc[0, :10])\n",
    "grad_means['Obese'] = list(means.iloc[1, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "mann = []\n",
    "\n",
    "for i in test:\n",
    "    ctrl = grad_stuff[i][grad_stuff['label'] == 'Ctrl']\n",
    "    fat = grad_stuff[i][grad_stuff['label'] == 'Obese']\n",
    "    mann.append(sp.stats.mannwhitneyu(ctrl, fat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MannwhitneyuResult(statistic=50021.0, pvalue=5.743462986209846e-29),\n",
       " MannwhitneyuResult(statistic=56065.0, pvalue=1.0373227210850577e-13),\n",
       " MannwhitneyuResult(statistic=53000.5, pvalue=4.4726913092891246e-14),\n",
       " MannwhitneyuResult(statistic=54095.0, pvalue=1.7819259764379774e-13),\n",
       " MannwhitneyuResult(statistic=74871.0, pvalue=0.0913742088911158),\n",
       " MannwhitneyuResult(statistic=56985.0, pvalue=3.5359461021296864e-11),\n",
       " MannwhitneyuResult(statistic=76291.5, pvalue=0.3969552609320985),\n",
       " MannwhitneyuResult(statistic=54231.0, pvalue=4.563468300691354e-13),\n",
       " MannwhitneyuResult(statistic=76269.0, pvalue=0.40894672831154855),\n",
       " MannwhitneyuResult(statistic=72018.0, pvalue=0.06459181295598002)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(len(grad_means))\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16,10))\n",
    "\n",
    "color = ['royalblue', 'orange']\n",
    "ax.barh(ind, grad_means.Ctrl, width, color = color[1], label = 'Ctrl', edgecolor ='black')\n",
    "ax.barh(ind + width, grad_means.Obese, width, color = color[0], label = 'Obese', edgecolor ='black')\n",
    "ax.set(yticks = ind + width/2, yticklabels = grad_means.Bacteria)\n",
    "#ax.set_title('Mea', fontsize = 18, fontweight='bold', pad=20)\n",
    "#ax.set_xscale('log')\n",
    "ax.set_ylabel('Species', fontsize = 18, fontweight='bold')\n",
    "ax.set_xlabel(\"Relative Abundance\", fontsize = 18, fontweight = 'bold', labelpad = 20)\n",
    "ax.legend(loc='lower right', fontsize = 'x-large')\n",
    "ax.tick_params(axis='both', labelsize = 14)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.savefig('Grad_impt_abundance.png', dpi=200, bbox_inches = \"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.plot(temp2['impt'])\n",
    "\n",
    "plt.xlabel('Species', fontsize = 16, fontweight = 'bold', labelpad=10)\n",
    "plt.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "plt.ylabel('Permutation Importance', fontsize = 16, fontweight = 'bold', labelpad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('grad_perm_impt_line.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Klebsiella pneumoniae',\n",
       " 'Bacteroides intestinalis',\n",
       " 'Dorea longicatena',\n",
       " 'Bacteroides stercoris',\n",
       " 'Bacteroides pectinophilus',\n",
       " 'Bacteroides massiliensis',\n",
       " 'Clostridium sp L2 50',\n",
       " 'Barnesiella intestinihominis',\n",
       " 'Streptococcus thermophilus',\n",
       " 'Ruminococcus obeum']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad_perm = perm_impt[['Features','Grad']].copy()\n",
    "\n",
    "grad_perm = grad_perm[grad_perm['Grad'] > 0].copy()\n",
    "\n",
    "# sort from highest to lowest importance\n",
    "grad_perm.sort_values(by='Grad', ascending=False, inplace=True, ignore_index=True)\n",
    "\n",
    "impt_features = grad_perm['Features'].copy()\n",
    "\n",
    "\n",
    "regex to shorten species names\n",
    "\n",
    "c = re.compile(r's__(\\w+)')\n",
    "\n",
    "short = []\n",
    "\n",
    "for i in grad_perm['Features']:\n",
    "    short.append( c.search(i).group(1).replace(\"_\", \" \"))\n",
    "    \n",
    "grad_perm['Features'] = short\n",
    "\n",
    "\n",
    "create a list of features we want from the original data\n",
    "stuff = list(processed.columns[:7]) + list(impt_features) + ['label']\n",
    "\n",
    "# filter the original dataset\n",
    "\n",
    "grad_stuff = processed[stuff].copy()\n",
    "\n",
    "# change bacteria name to be shorter\n",
    "new = dict(zip(impt_features, short))\n",
    "\n",
    "grad_stuff.rename(columns = new, inplace=True)\n",
    "\n",
    "# changing labels for control and obese\n",
    "grad_stuff['label'][grad_stuff['label'] == 1] = 'Obese'\n",
    "\n",
    "grad_stuff['label'][grad_stuff['label'] == 0] = 'Ctrl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate mean species abundance for each species\n",
    "\n",
    "means = grad_stuff.groupby(by = 'label').mean().iloc[:,1:]\n",
    "\n",
    "# create a dataframe for mean abundance\n",
    "\n",
    "grad_means = pd.DataFrame(columns = ['Bacteria','Ctrl','Obese'])\n",
    "\n",
    "grad_means['Bacteria'] = short\n",
    "grad_means['Ctrl'] = list(means.iloc[0, :])\n",
    "grad_means['Obese'] = list(means.iloc[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots permutation importance and abundance\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,10))\n",
    "\n",
    "\n",
    "ax[0].plot(grad_perm['Grad'])\n",
    "ax[0].set_xlabel('Species', fontsize=18, labelpad = 10)\n",
    "ax[0].set_ylabel('Permutation importance', fontsize=18, labelpad = 10)\n",
    "ax[0].tick_params(axis=\"both\", labelsize=14)\n",
    "\n",
    "ax[1].scatter([range(len(grad_perm))], grad_means['Ctrl'])\n",
    "ax[1].scatter([range(len(grad_perm))], grad_means['Obese'])\n",
    "ax[1].set_xlabel('Species', fontsize=18, labelpad = 10)\n",
    "ax[1].set_ylabel('Species Abundance', fontsize=18, labelpad = 5)\n",
    "ax[1].tick_params(axis=\"both\", labelsize=14)\n",
    "ax[1].legend(['Control','Obese'], fontsize = 'xx-large')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grad_perm_importance.png', dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison plots for the 11 common ones ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one basically finds the common species in all models\n",
    "common = [i for i in species if (i in forest_impt) and (i in grad_impt) and (i in ada_impt)]\n",
    "\n",
    "# this one is to remove the s__ in the species names\n",
    "c = re.compile(r's__(\\w+)')\n",
    "\n",
    "short = []\n",
    "\n",
    "for i in common:\n",
    "    short.append( c.search(i).group(1).replace(\"_\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filers the feature importance table to see the 8 common species got what coefficient\n",
    "\n",
    "common_impt = perm_impt[perm_impt['Features'].isin(common)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new table from the original table that consists of the 8 common bacteria and the other info\n",
    "\n",
    "common_only = processed[(list(processed.columns[:7]) + common + ['label'])]\n",
    "\n",
    "common_only['label'][common_only['label'] == 1] = 'Obese'\n",
    "\n",
    "common_only['label'][common_only['label'] == 0] = 'Ctrl'\n",
    "\n",
    "# rename the columns to the shortened bacteria name \n",
    "common_only.rename(columns = dict(zip(common_only.columns[7:-1], short)), inplace =True)\n",
    "\n",
    "\n",
    "# create 2 dataframes with mean and std dev for each species and BMI\n",
    "\n",
    "means = common_only.groupby(by = 'label').mean().iloc[:,1:]\n",
    "error = common_only.groupby(by = 'label').std().iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  creates a dataframe with species name, then the mean species abundance for controls and obese groups then plot a barchart\n",
    "\n",
    "compare_means = pd.DataFrame(columns = ['Bacteria','Ctrl','Obese'])\n",
    "\n",
    "compare_means['Bacteria'] = short\n",
    "compare_means['Ctrl'] = list(means.iloc[0, :])\n",
    "compare_means['Obese'] = list(means.iloc[1, :])\n",
    "\n",
    "# compare_means.plot(x = 'Bacteria', y = ['Ctrl', 'Obese'], kind='barh', title = 'All', logx=True, figsize = (14,8), \n",
    "#                    xlabel = 'Mean Relative Abundance', ylabel = 'Bacteria', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_summary(common_only, list(common_only['country']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_summary(common_only, ['china'], name= \"China\", filename = 'china_8_common.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions calculates the mean of the relative abundance and plots barcharts. \n",
    "# Take note that it uses some variables that were declared earlier, so it is not entirely standalone. No need to remove the categorical stuff\n",
    "# since groupby removes them\n",
    "\n",
    "def create_summary(dataframe, group, name = 'Unnamed', filename = ('Unnamed' + '.png'), color = ['royalblue', 'orange']):\n",
    "\n",
    "    #create new DF with the group of interest\n",
    "    grouped = dataframe[dataframe['country'].isin(group)].copy()\n",
    "    \n",
    "    # create 2 dataframes with mean and std dev for each species and BMI\n",
    "\n",
    "    g_means = grouped.groupby(by = 'label').mean().iloc[:,1:]\n",
    "    g_error = grouped.groupby(by = 'label').std().iloc[:,1:]\n",
    "\n",
    "    #  creates a dataframe with species name, then the mean species abundance for controls and obese groups then plot a barchart\n",
    "\n",
    "    grouped_means = pd.DataFrame(columns = ['Bacteria','Ctrl','Obese'])\n",
    "\n",
    "    grouped_means['Bacteria'] = short\n",
    "    grouped_means['Ctrl'] = list(g_means.iloc[0, :])\n",
    "    grouped_means['Obese'] = list(g_means.iloc[1, :])\n",
    "\n",
    "    ind = np.arange(len(grouped_means))\n",
    "\n",
    "    width = 0.4\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (16,10))\n",
    "\n",
    "\n",
    "    ax.barh(ind, grouped_means.Obese, width, color = color[0], label = 'Obese')\n",
    "    ax.barh(ind + width, grouped_means.Ctrl, width, color = color[1], label = 'Ctrl')\n",
    "    ax.set(yticks = ind + width, yticklabels = grouped_means.Bacteria)\n",
    "    ax.set_title(name, fontsize = 18, fontweight='bold', pad=20)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Species', fontsize = 18, fontweight='bold')\n",
    "    ax.set_xlabel(\"Log Relative Abundance\", fontsize = 18, fontweight = 'bold', labelpad = 20)\n",
    "    ax.legend(loc='lower right', fontsize = 'x-large')\n",
    "    ax.tick_params(axis='both', labelsize = 14)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    #plt.savefig(filename, dpi=150, bbox_inches = \"tight\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_summary(common_only, ['china'], name= \"China\", filename = 'china_8_common.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest feature analysis ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = processed.iloc[:, 7:-1].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(test, xticklabels=False, yticklabels =False, cmap = cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dataset_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe that contains all impt features from random forest model and sort them\n",
    "\n",
    "forest_perm = perm_impt[['Features','Forest']].copy()\n",
    "\n",
    "forest_perm = forest_perm[forest_perm['Forest'] > 0].copy()\n",
    "\n",
    "forest_perm.sort_values(by='Forest', ascending=False, inplace=True, ignore_index=True)\n",
    "\n",
    "impt_features = forest_perm['Features'].copy()\n",
    "\n",
    "\n",
    "# regex to shorten species names\n",
    "\n",
    "c = re.compile(r's__(\\w+)')\n",
    "\n",
    "short = []\n",
    "\n",
    "for i in forest_perm['Features']:\n",
    "    short.append( c.search(i).group(1).replace(\"_\", \" \"))\n",
    "\n",
    "forest_perm['Features'] = short\n",
    "\n",
    "# create a list of features we want from the original data\n",
    "stuff = list(processed.columns[:7]) + list(impt_features) + ['label']\n",
    "\n",
    "# filter the original dataset\n",
    "\n",
    "forest_stuff = processed[stuff].copy()\n",
    "\n",
    "# change bacteria name to be shorter\n",
    "new = dict(zip(impt_features, short))\n",
    "\n",
    "forest_stuff.rename(columns = new, inplace=True)\n",
    "\n",
    "# changing labels for control and obese\n",
    "forest_stuff['label'][forest_stuff['label'] == 1] = 'Obese'\n",
    "\n",
    "forest_stuff['label'][forest_stuff['label'] == 0] = 'Ctrl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean species abundance for each species\n",
    "\n",
    "means = forest_stuff.groupby(by = 'label').mean().iloc[:,1:]\n",
    "\n",
    "# create a dataframe for mean abundance\n",
    "\n",
    "forest_means = pd.DataFrame(columns = ['Bacteria','Ctrl','Obese'])\n",
    "\n",
    "forest_means['Bacteria'] = short\n",
    "forest_means['Ctrl'] = list(means.iloc[0, :])\n",
    "forest_means['Obese'] = list(means.iloc[1, :])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dual plot of perm importance and species abundance\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,10))\n",
    "\n",
    "\n",
    "ax[0].plot(forest_perm['Forest'])\n",
    "ax[0].set_xlabel('Species', fontsize=18, labelpad = 10)\n",
    "ax[0].set_ylabel('Permutation importance', fontsize=18, labelpad = 10)\n",
    "ax[0].tick_params(axis=\"both\", labelsize=14)\n",
    "\n",
    "ax[1].scatter([range(35)], forest_means['Ctrl'])\n",
    "ax[1].scatter([range(35)], forest_means['Obese'])\n",
    "ax[1].set_xlabel('Species', fontsize=18, labelpad = 10)\n",
    "ax[1].set_ylabel('Species Abundance', fontsize=18, labelpad = 5)\n",
    "ax[1].tick_params(axis=\"both\", labelsize=14)\n",
    "ax[1].legend(['Control','Obese'], fontsize = 'xx-large')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('forest_perm_importance.png', dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot barchart\n",
    "\n",
    "create_summary(forest_stuff, list(forest_stuff['country'].unique()), name='forest')\n",
    "\n",
    "eu = ['denmark', 'spain', 'sweden', 'germany','france', 'iceland']\n",
    "\n",
    "# create_summary(forest_stuff, eu, name='forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find the common and different points\n",
    "\n",
    "# create dataframe of the EU and CN data and obtain their means\n",
    "eu_only = forest_stuff[forest_stuff['country'].isin(eu)]\n",
    "\n",
    "cn_only = forest_stuff[forest_stuff['country'].isin(['china'])]\n",
    "\n",
    "eu_mean = eu_only.groupby(by='label').mean().drop(columns = 'bmi').transpose().reset_index()\n",
    "\n",
    "cn_mean = cn_only.groupby(by='label').mean().drop(columns = 'bmi').transpose().reset_index()\n",
    "\n",
    "eu_mean['ctrl_larger'] = eu_mean['Ctrl'] > eu_mean['Obese']\n",
    "\n",
    "cn_mean['ctrl_larger'] = cn_mean['Ctrl'] > cn_mean['Obese']\n",
    "\n",
    "# agreement dataframe\n",
    "\n",
    "eu_mean['agreement'] = eu_mean['ctrl_larger'] & cn_mean['ctrl_larger']\n",
    "\n",
    "agreed = eu_mean[['index','agreement']][eu_mean['agreement'] == True].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common and differences for the EU countries\n",
    "\n",
    "den = forest_stuff[forest_stuff['country'].isin(['denmark'])]\n",
    "\n",
    "#france = forest_stuff[forest_stuff['country'].isin(['france'])]\n",
    "\n",
    "den_mean = den.groupby(by='label').mean().drop(columns = 'bmi').transpose().reset_index()\n",
    "\n",
    "#france_mean = france.groupby(by='label').mean().drop(columns = 'bmi').transpose().reset_index()\n",
    "\n",
    "den_mean['ctrl_larger'] = den_mean['Ctrl'] > den_mean['Obese']\n",
    "\n",
    "#france_mean['ctrl_larger'] = france_mean['Ctrl'] > france_mean['Obese']\n",
    "\n",
    "# agreement dataframe\n",
    "\n",
    "den_mean['agreement'] = den_mean['ctrl_larger'] & cn_mean['ctrl_larger']\n",
    "\n",
    "#agreed = eu_mean[['index','agreement']][eu_mean['agreement'] == True].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new DF with the group of interest\n",
    "\n",
    "# grouped = forest_stuff[ list(agreed['index']) + ['label'] ]\n",
    "\n",
    "# create 2 dataframes with mean and std dev for each species and BMI\n",
    "\n",
    "# g_means = grouped.groupby(by='label').mean().iloc[:,:]\n",
    "# g_error = grouped.groupby(by= 'label').std().iloc[:,:]\n",
    "\n",
    "#  creates a dataframe with species name, then the mean species abundance for controls and obese groups then plot a barchart\n",
    "\n",
    "# grouped_means = pd.DataFrame(columns = ['Bacteria','Ctrl','Obese'])\n",
    "\n",
    "# grouped_means['Bacteria'] = list(grouped.columns[:-1])\n",
    "# grouped_means['Ctrl'] = list(g_means.iloc[0, :])\n",
    "# grouped_means['Obese'] = list(g_means.iloc[1, :])\n",
    "\n",
    "ind = np.arange(len(grouped_means))\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16,10))\n",
    "\n",
    "color = ['royalblue', 'orange']\n",
    "ax.barh(ind, grouped_means.Obese, width, color = color[0], label = 'Obese')\n",
    "ax.barh(ind + width, grouped_means.Ctrl, width, color = color[1], label = 'Ctrl')\n",
    "ax.set(yticks = ind + width, yticklabels = grouped_means.Bacteria)\n",
    "ax.set_title('test', fontsize = 18, fontweight='bold', pad=20)\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Species', fontsize = 18, fontweight='bold')\n",
    "ax.set_xlabel(\"Log Relative Abundance\", fontsize = 18, fontweight = 'bold', labelpad = 20)\n",
    "ax.legend(loc='lower right', fontsize = 'x-large')\n",
    "ax.tick_params(axis='both', labelsize = 14)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# #plt.savefig(filename, dpi=150, bbox_inches = \"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_mean['label'][eu_mean['agreement']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try comparing importance with abundance\n",
    "\n",
    "forest_perm['relative_impt'] = forest_perm['Forest'].div(forest_perm['Forest'].sum(), axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
