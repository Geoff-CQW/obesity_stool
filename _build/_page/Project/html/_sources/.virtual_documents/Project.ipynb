### import packages ###

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re

### import raw data

data = pd.read_csv('metagenomics/abundance_stoolsubset.csv', dtype='str')
cols = data.columns

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn


# Let's take a look at the raw data

print('sample dimensions are {} rows by {} columns'.format(data.shape[0], data.shape[1]))
print()
data.head(2)


### preprocess data ###

# filter for categories of interest

processed = data[data['disease'].isin(['obesity', 'obese', 'overweight','leaness', 'n'])].copy()

# drop columns that are not needed
to_drop = list(cols[2:4]) + list(cols[8:20]) + list(cols[21:211])
processed.drop(columns = to_drop, inplace = True)

# remove samples without bmi, convert bmi and abundance to float and filter for regular weight and obese
processed = processed[~processed['bmi'].isin(['na', 'nd'])]

processed['bmi'] = pd.to_numeric(processed['bmi'], downcast ='float')

# processed.iloc[:, 7:] = processed.iloc[:, 7:].apply(pd.to_numeric)

processed.iloc[:, 7:] = processed.iloc[:, 7:].astype(np.float32)
# create labels for classification

bmi = [(processed['bmi'] >= 18.5) & (processed['bmi'] < 25), (processed['bmi'] >= 25) & (processed['bmi'] < 30), processed['bmi'] >= 30 ]

categories = ['1','2','3']

classes = np.select(bmi, categories, default = '0')

np.unique(classes, return_counts=True)


bmi = [(processed['bmi'] >= 18.5) & (processed['bmi'] < 25), processed['bmi'] >= 30 ]

categories = ['0','1']

processed['bmi_category'] = np.select(bmi, categories, default = '2')


processed.iloc[5, 7:16]


# get names of all columns that contain abundance data
bacteria = list(processed.columns)[7:-1]

# use regex to select exclusively for columns that contain species level abundance

# filters for species level
s = re.compile(r'(\|s__\w+$)')

# filter for genus level
g = re.compile(r'\|g__\w+$')


not_species = [i for i in bacteria if not s.search(i)]
not_genus = [j for j in bacteria if not g.search(j)]

# drop columns that are not needed
species_data = processed[processed['bmi_category'] != '2'].drop(columns = not_species)

genus_data = processed[processed['bmi_category'] != '2'].drop(columns = not_genus)

print('New dimensions of the dataset:')
print()
print('Species level dataset: {} rows by {} cols'.format(species_data.shape[0], species_data.shape[1]))
print()
print('Genus level dataset: {} rows by {} cols'.format(genus_data.shape[0], genus_data.shape[1]))



# rename feature names to shorter ones for species

s = re.compile(r's__(\w+)')

short = []

for i in list(species_data)[7:-1]:
    short.append(s.search(i).group(1).replace("_", " "))
    
new = dict(zip(species_data, list(species_data)[:7] + short))

species_data.rename(columns = new, inplace=True)

# same thing for genus

g = re.compile(r'g__(\w+)')

short = []

for i in list(genus_data)[7:-1]:
    short.append(g.search(i).group(1).replace("_", " "))
    
new = dict(zip(genus_data, list(genus_data)[:7] + short))

genus_data.rename(columns = new, inplace=True)


# quick and dirty missing value check

print('there are {} missing values in species and {} in genus.'.format(species_data.isna().sum().sum(), genus_data.isna().sum().sum()))


# a list to collect index of observations that add up to 0 or more than 100
gt = []
lt = []
for i in range(len(species_data)):
    if (species_data.iloc[i, 7:-1].sum() > 100):
        gt.append(i)
    elif species_data.iloc[i, 7:-1].sum() < 100:
        lt.append(i)

print('{} observations >100% and {} observations <100% to check'.format(len(gt), len(lt)))



for i in np.random.randint(0, len(lt), size=10):
    print('row {}, total abundance {}'.format(lt[i], species_data.iloc[lt[i], 7:-1].sum()))



for i in np.random.randint(0, len(gt), size=10):
    print('row {}, total abundance {}'.format(gt[i], species_data.iloc[gt[i], 7:-1].sum()))


def calculate_mann_whitney(df, column_name):
    '''Calculates mann_whitney U rank test rank-biserial correlation
    Assumes the data has only 2 classes, 1 and 0
    and the last column contains the classes'''

    from scipy.stats import mannwhitneyu
    
    # dictionary to collect data
    variables = {}

    for i in range(len(df.columns)-1):
        class0 = df[df.columns[i]][df[column_name] == '0']
        class1 = df[df.columns[i]][df[column_name] == '1']

        u = mannwhitneyu(class0, class1)[0]

        # the calculation of rank-biserial correlation changes depending on whether the test statistic, U, is the smaller one
        # since scipy always returns the test statistic of the first argument, rather than do 2 Mann-Whitney tests to get
        # both U1 and U2 to compare. I'll just stick to one calculation and absolute it to get the correlation regardless
        # of whether U is larger or smaller 
        variables[df.columns[i]]= np.absolute((2 * u)/(len(class0)*len(class1)) - 1)

    return pd.DataFrame.from_dict(variables, orient='index', columns = ['rank_biserial'])


species_mann = calculate_mann_whitney(species_data.iloc[:, 7:], 'bmi_category')

plt.figure(figsize=(10,6), dpi=100)
sns.scatterplot(data=species_mann, y = 'rank_biserial', x = species_mann.index.values)
plt.ylabel('Correlation')
plt.xticks(ticks = [])


fig = plt.figure(figsize=(20,10), dpi=150)

topSP = species_mann.sort_values(by='rank_biserial', ascending=False).index.values[:10]

top_subset = pd.concat([(species_data[topSP] + 0.00001), species_data['bmi_category']], axis=1)

for i in range(10):
    fig.add_subplot(2, 5, i+1)
    sns.histplot(data= top_subset, x = topSP[i], hue='bmi_category', log_scale=10)
    # plt.xlim(left = 0.001)

plt.suptitle('Relative abundance of species with highest correlation (descending)', fontsize=16)
fig.set_size_inches(20, 10)
plt.tight_layout()


from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn import metrics
from sklearn.metrics import balanced_accuracy_score

X = species_data.iloc[:, 7:species_data.shape[1]-1]
y = species_data.iloc[:, -1]

# split data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 77)

# scale data based on training set
scaler = StandardScaler().fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)



# simple function to output the performance metrics of a model

def model_performance(y_true, y_pred):
    '''prints model metrics'''
    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

    fig = plt.figure(dpi=100)

    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, ax=fig.gca(), display_labels=['normal','obese'])

    print(classification_report(y_true, y_pred))
    # print('balanced accuracy: {0:.4g}'.format(metrics.balance_accuracy_score(y_true, y_pred)))


# and another one to collate the scores
def model_comparison(models, names, x_test1, y_test, pls=None):
    '''Returns a dataframe of various classification metrics

    Parameters: 
    models: trained models
    names: names for models in dataframe
    x_test1: test data
    y_test: test classes
    pls: pls model, which is used to transform x_test before prediction
    
    '''

    assert len(models) == len(names), "number of models and names is not the same"

    model_scores = {}
   
    for i in range(len(names)):
        
        temp = {}
        
        if (names[i] == 'PLS-DA') :
            x_test = pls.transform(x_test1)
        else:
            x_test = x_test1

        temp['Balanced Accuracy'] = metrics.balanced_accuracy_score(y_test, models[i].predict(x_test))
        temp['ROC AUC'] = metrics.roc_auc_score(y_test, models[i].predict(x_test).astype(np.object))
        temp['Precision 1'] = metrics.precision_score(y_test, models[i].predict(x_test), pos_label = '1')
        temp['Precision 0'] = metrics.precision_score(y_test, models[i].predict(x_test), pos_label = '0')
        temp['Recall 1'] = metrics.recall_score(y_test, models[i].predict(x_test), pos_label = '1')
        temp['Recall 0'] = metrics.recall_score(y_test, models[i].predict(x_test), pos_label = '0')

        # create a dictionary for the current model
        model_scores[names[i]] = temp


        #tabulate metrics
    return pd.DataFrame.from_dict(model_scores, orient='index').round(3)



from sklearn.linear_model import LogisticRegression
from scipy.stats import uniform, norm
sns.set_theme(style='white')

log_params = dict(C=uniform(loc=0.0, scale=20.0), penalty = ['l1', 'l2'])

# fit log regression
log_reg = LogisticRegression(random_state=78, max_iter=1000, solver='liblinear')

# randomized hyperparameter search
log_reg_CV = RandomizedSearchCV(log_reg, log_params, n_iter = 50, n_jobs=6, random_state = 78, scoring = 'balanced_accuracy', refit=True).fit(x_train, y_train)

log_reg_CV.best_params_

model_performance(y_test,log_reg_CV.predict(x_test))


from sklearn.cross_decomposition import PLSRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

n_components = []
score = []

for i in np.arange(2,82,4):

    # pls model
    plsR = PLSRegression(n_components=i, scale=False).fit(x_train, y_train)

    # use components for LDA
    lda = LinearDiscriminantAnalysis().fit(plsR.x_scores_, y_train)

    n_components.append(i)
    score.append(metrics.balanced_accuracy_score(y_test, lda.predict(plsR.transform(x_test))))

plt.plot(n_components, score)

print('no. of components for best score: {}'.format(n_components[score.index(max(score))]))


plsR = PLSRegression(n_components=6, scale=False).fit(x_train, y_train)

lda = LinearDiscriminantAnalysis().fit(plsR.x_scores_, y_train)

model_performance(y_test, lda.predict(plsR.transform(x_test)))

# to look at the species with highest absolute loadings
#species_data.columns[np.argsort(np.absolute(plsR.x_loadings_[:, 0]))[:10]]


# get indices of features with correlation 

idx = np.where(species_mann != 0.0)[0]

print("{} features with non-zero rank-biserial correlation".format(len(idx)))


# everything stays the same, except training and test data dimension is now reduced

log_params = dict(C=uniform(loc=0.0, scale=20.0), penalty = ['l1', 'l2'])

# fit log regression
rank_log_reg = LogisticRegression(random_state=78, max_iter=1000, solver='liblinear')

# randomized hyperparameter search
rank_log_reg_CV = RandomizedSearchCV(rank_log_reg, log_params, n_iter = 50, n_jobs=6, random_state = 78, scoring = 'balanced_accuracy', refit=True).fit(x_train[:, idx], y_train)


# random feature selection

rand_idx = [np.random.randint(0, x_train.shape[1], 609) for _ in range(3)]

rand_log_params = dict(C=uniform(loc=0.0, scale=20.0), penalty = ['l1', 'l2'])

# fit log regression
rand_log = LogisticRegression(random_state=78, max_iter=1000, solver='liblinear')

rand_acc = []

for i in rand_idx:
    rand_log_CV = RandomizedSearchCV(rand_log, rand_log_params, n_iter = 50, random_state = 78, n_jobs=6, scoring = 'balanced_accuracy', refit=True).fit(x_train[:, i], y_train)
    rand_acc.append(metrics.balanced_accuracy_score(y_test, rand_log_CV.predict(x_test[:, i])))


print("average difference in accuracy: {}".format(np.mean(metrics.balanced_accuracy_score(y_test, rank_log_reg_CV.predict(x_test[:, idx])) - np.array(rand_acc))))


model_performance(y_test,rank_log_reg_CV.predict(x_test[:, idx]))


#step by step optimization

from sklearn.svm import SVC

svm_params = {'kernel':['rbf','sigmoid','poly'], 'C':uniform(loc=0, scale=10)}

svm = SVC(max_iter = 5000, random_state=78)

svm_cv = RandomizedSearchCV(svm, svm_params, scoring = 'balanced_accuracy', random_state = 78, n_jobs = 6, refit=True).fit(x_train, y_train)

svm_cv.best_params_

model_performance(y_test, svm_cv.predict(x_test))


from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint

forest = RandomForestClassifier(random_state=78)

forest_params = {'n_estimators':randint(0, 50), 'max_features':['sqrt','log2']}

forest_cv = RandomizedSearchCV(forest, forest_params, scoring = 'balanced_accuracy', random_state = 78, n_jobs=6, refit=True).fit(x_train, y_train)

forest_cv.best_params_

model_performance(y_test, forest_cv.predict(x_test))


from sklearn.ensemble import AdaBoostClassifier


ada = AdaBoostClassifier(random_state=78)

ada_params = {'n_estimators':randint(0, 200)}

ada_cv = RandomizedSearchCV(ada, ada_params, scoring = 'balanced_accuracy', n_jobs=6, random_state = 78, refit=True).fit(x_train, y_train)

model_performance(y_test, ada_cv.predict(x_test))


from sklearn.ensemble import GradientBoostingClassifier

grad = GradientBoostingClassifier(random_state=78)

grad_params = {'n_estimators':randint(0, 300)}

grad_cv = RandomizedSearchCV(grad, grad_params, n_jobs=8, random_state = 78, refit=True).fit(x_train, y_train)

grad_cv.best_params_

model_performance(y_test, grad_cv.predict(x_test))


models = [log_reg_CV, lda, svm_cv, forest_cv, ada_cv, grad_cv]

names = ['log regression', 'PLS-DA','kernel SVM', 'Random Forest', "Adaboost",'Gradient Boost']

model_comparison(models, names, x_test, y_test, plsR)



models = [forest_cv, ada_cv, grad_cv]

names = ['Random Forest', "Adaboost",'Gradient Boost']

fig, ax = plt.subplots(1,2, figsize=(12,6), dpi=100)

for i in range(len(names)):

    metrics.plot_roc_curve(models[i], x_test, y_test, ax=ax[0], name = names[i])
    ax[0].set_title('ROC', fontsize=16,fontweight='demibold')
    # ax[0].tick_params(axis='both', labelsize=14)
    
    metrics.plot_precision_recall_curve(models[i], x_test, y_test, ax=ax[1], name = names[i])
    ax[1].set_title('Precision-Recall', fontsize=16,fontweight='demibold')
    # ax[1].tick_params(axis='both', labelsize=14)

plt.tight_layout()


### permutation importance ###

from sklearn.inspection import permutation_importance
  
perm_impt = permutation_importance(ada_cv, x_test, y_test, scoring='balanced_accuracy', n_repeats=10, n_jobs=6)

perm_idx = perm_impt.importances_mean.argsort()[::-1]

print('Top 10 species by feature importance (descending):')

pd.DataFrame({'species':X.columns[perm_idx][:10], 'permutation importance':perm_impt.importances_mean[perm_idx][:10]})



fig = plt.figure(figsize=(20,10), dpi=150)

impt_SP = species_data.columns[7:][perm_idx[:10]].values

impt_subset = pd.concat([(species_data[impt_SP] + 0.00001), species_data['bmi_category']], axis=1)

for i in range(10):
    fig.add_subplot(2, 5, i+1)
    sns.histplot(data= impt_subset, x = impt_SP[i], hue='bmi_category', log_scale=10)
    # plt.xlim(left = 0.001)

plt.tight_layout()


print('Species with high rank-biserial correlation and permutation importance:')

[x for x in impt_SP if x in topSP]


X = genus_data.iloc[:, 7:genus_data.shape[1]-1]
y = genus_data.iloc[:, -1]

# split data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 878)

# scale data based on training set
scaler = StandardScaler().fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

ada = AdaBoostClassifier(random_state=78)

ada_params = {'n_estimators':randint(0, 200)}

ada_cv = RandomizedSearchCV(ada, ada_params, scoring = 'balanced_accuracy', n_jobs=6, random_state = 78, refit=True).fit(x_train, y_train)

model_performance(y_test, ada_cv.predict(x_test))
